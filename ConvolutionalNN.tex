% !TEX root = Main.tex
\section{Neural Networks}
\textbf{Neurons}: $F_\sigma(\mathbf{x};\mathbf{w}) = \sigma(w_0 + \sum_{i=1}^M{x_iw_i})$.\\
\textbf{Activation}: $s(x)=\frac{1}{1+e^{-x}}$, $s'(x)=s(x)(1-s(x))$; $tanh(x) = \tfrac{sinh(x)}{cosh(x)}$, $tanh'(x) = 1 - tanh^2(x)$;  $ReLU(x) = max(0, x)$, $ReLU'(x) = 0\  or\  1$\\
\textbf{Output}: Linear Regression: $\hat{\mathbf{y}} = \mathbf{W}^L\mathbf{x}^{L-1}$\\
Binary Classification (Logistic):\\
$\hat{y_1} = \text{P}[Y=1|\mathbf{x}] = \frac{1}{1 + \exp[-\langle \mathbf{w}_1^L,\mathbf{x}^{L-1}\rangle]}$\\
Multiclass (Softmax):\\
$\hat{y_k} = \text{P}[Y=k|\mathbf{x}]= \frac{\exp[\langle \mathbf{w}_k^L,\mathbf{x}^{L-1}\rangle]}{\sum_{m=1}^{K}{\exp[\langle \mathbf{w}_m^L, \mathbf{x}^{L-1}\rangle]}}$.\\
(softmax: $\sigma(z)_j = e^{z_j}/\sum_{k=1}^K e^{z_k}$ for $j=1,\dots,K$)\\
\textbf{Loss Function:} Squared Loss $\frac{1}{2}(y - \hat{y})^2$\\
Cross-Entropy Loss: $-y \log \hat{y} - (1-y)\log(1-\hat{y})$.\\
\textbf{Units \& Layers}: FW Prop: $\mathbf{x}^{l} = \sigma^{l}\left(\mathbb{W}^{\left(l\right)}\mathbf{x}^{\left(l-1\right)}\right)$. L-layer network: $\mathbf{y}=\sigma^{\left(L\right)}\left(\mathbf{W}^{(L)}\sigma^{(L-1)}\left(\cdots\left(\sigma^{(1)}\left(\mathbf{W}^{(1)}\mathbf{x}\right)\cdots\right)\right)\right)$

\subsection*{Backpropagation (L2L Jacobian)}
$\mathbf{x}$ = Prev Layer Act, $\mathbf{x^+}$ = Curr Layer Act\\$\mathbf{J}$ = $J_{ij}$ (Jacobian of mapping $\mathbf{x}\rightarrow\mathbf{x^+}$)\\ $\mathbf{x_i^+} = \sigma(\mathbf{w}_i^\top\mathbf{x})$, $J_{ij} = \frac{\partial \mathbf{x_i^+}}{\partial \mathbf{x}_j} = w_{ij}\cdot\sigma'(\mathbf{w}_i^\top\mathbf{x})$.\\
Multi-Layer (First FW, then BW):\\
$\frac{\partial\mathbf{x}^{(l)}}{\partial\mathbf{x}^{(l-n)}} = \mathbf{J}^{(l)}\cdot\frac{\partial\mathbf{x}^{(l-1)}}{\partial\mathbf{x}^{(l-n)}}=\mathbf{J}^{(l)}\cdot\mathbf{J}^{(l-1)}\cdots\mathbf{J}^{(l-n+1)}$, then BP $ \nabla_{x^{(l)}}^\top\ell=\nabla_{y}^\top\ell\cdot\mathbf{J}^{(L)}\cdots\mathbf{J}^{(l+1)}$

\subsection*{Convolutional Neural Networks}
Translation invariance of images $\rightarrow$ neurons compute same func, shift invariant filters; weights defined as filter masks, e.g. convolution: $F_{n,m}(\mathbf{x};\mathbf{w}) = \sigma(b + \sum_{k=-2}^2\sum_{l=-2}^{2}{w_{k,l}x_{n+k,m+l}})$.\\
$i',j'$ win. pos, $k$: win. size, $K$: weigths, $c$: chan.:\\
 $(I\star K^l)_{i', j'} = \sum_c \sum_{-k\leq i,j \leq k} pad(I_c)_{i'+i,j'+j} \cdot(K_c^l)_{i,j}$\\
Max-Pooling (size 3): $\max_{-1\leq i', j' \leq 1}(\cdot)_{i+i', j+j'}$ % TODO: Make prettier, remove $\cdot$ 

\subsection*{ELBO}
$\lambda^* = argmax_\lambda ELBO(\lambda)$ where $ELBO=$\\
$log p(y,\theta) \geq E_q[log p(x^{1:L},y; \theta)-log q(x^{1:L}, y)]$

\subsection*{Autoregressive models}
$p(x)=\pi_{i=1}^{n^2} p(x_i|x_1,\dots ,x_{i-1})$